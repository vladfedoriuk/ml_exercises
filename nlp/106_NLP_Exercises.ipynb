{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercises\n",
    "\n",
    "We have five exercises in this section. The exercises are:\n",
    "1. Build your own tokenizer, where you need to implement two functions to implement a tokenizer based on regular expression.\n",
    "2. Get tags from Trump speech.\n",
    "3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "4. Build your own Bag Of Words implementation using tokenizer created before.\n",
    "5. Build a 5-gram model and clean up the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Build your own tokenizer\n",
    "\n",
    "Build two different tokenizers:\n",
    "- ``tokenize_sentence``: function tokenizing text into sentences,\n",
    "- ``tokenize_word``: function tokenizing text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "['Here we go again', ' I was supposed to add this text later', \"Well, it's 10.p.m. here, and I'm actually having fun making this course. :oI hope you are getting along fine with this presentation, I really did try\", 'And one last sentence, just so you can test you tokenizers better.']\n",
      "Tokenized words:\n",
      "['Here', 'we', 'go', 'again', 'I', 'was', 'supposed', 'to', 'add', 'this', 'text', 'later', 'Well', 'it', 's', '10', 'p', 'm', 'here', 'and', 'I', 'm', 'actually', 'having', 'fun', 'making', 'this', 'course', 'oI', 'hope', 'you', 'are', 'getting', 'along', 'fine', 'with', 'this', 'presentation', 'I', 'really', 'did', 'try', 'And', 'one', 'last', 'sentence', 'just', 'so', 'you', 'can', 'test', 'you', 'tokenizers', 'better']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def tokenize_words(text: str) -> list:\n",
    "    \"\"\"Tokenize text into words using regex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "            Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "            List containing words tokenized from text\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(filter(lambda x: len(x), re.split(r'[^(\\b\\w+\\b)]', text)))\n",
    "\n",
    "def tokenize_sentence(text: str) -> list:\n",
    "    \"\"\"Tokenize text into words using regex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "            Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "            List containing words tokenized from text\n",
    "\n",
    "    \"\"\"\n",
    "    return re.split('[\\.!?]+(?=\\s?[A-Z0-9])', text)\n",
    "\n",
    "text = \"Here we go again. I was supposed to add this text later.\\\n",
    "Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\\\n",
    "I hope you are getting along fine with this presentation, I really did try.\\\n",
    "And one last sentence, just so you can test you tokenizers better.\"\n",
    "\n",
    "print(\"Tokenized sentences:\")\n",
    "print(tokenize_sentence(text))\n",
    "\n",
    "print(\"Tokenized words:\")\n",
    "print(tokenize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Get tags from Trump speech using NLTK\n",
    "\n",
    "You should use the ``trump.txt`` file, read it and find the tags for each word. Use NLTK for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thank', 'NOUN'),\n",
       " ('you', 'PRON'),\n",
       " ('very', 'ADV'),\n",
       " ('much', 'ADV'),\n",
       " ('.', '.'),\n",
       " ('Mr.', 'NOUN'),\n",
       " ('Speaker', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Mr.', 'NOUN'),\n",
       " ('Vice', 'NOUN'),\n",
       " ('President', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Members', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Congress', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('First', 'NOUN'),\n",
       " ('Lady', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('United', 'NOUN'),\n",
       " ('States', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('citizens', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('America', 'NOUN'),\n",
       " (':', '.'),\n",
       " ('Tonight', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('as', 'ADP'),\n",
       " ('we', 'PRON'),\n",
       " ('mark', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('conclusion', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('celebration', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Black', 'NOUN'),\n",
       " ('History', 'NOUN'),\n",
       " ('Month', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('we', 'PRON'),\n",
       " ('are', 'VERB'),\n",
       " ('reminded', 'VERB'),\n",
       " ('of', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('Nation', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('path', 'NOUN'),\n",
       " ('towards', 'NOUN'),\n",
       " ('civil', 'ADJ'),\n",
       " ('rights', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('work', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('still', 'ADV'),\n",
       " ('remains', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('done', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('Recent', 'ADJ'),\n",
       " ('threats', 'NOUN'),\n",
       " ('targeting', 'VERB'),\n",
       " ('Jewish', 'NOUN'),\n",
       " ('community', 'NOUN'),\n",
       " ('centers', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('vandalism', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Jewish', 'ADJ'),\n",
       " ('cemeteries', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('as', 'ADV'),\n",
       " ('well', 'ADV'),\n",
       " ('as', 'ADP'),\n",
       " ('last', 'ADJ'),\n",
       " ('week', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('shooting', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('Kansas', 'NOUN'),\n",
       " ('City', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('remind', 'VERB'),\n",
       " ('us', 'PRON'),\n",
       " ('that', 'ADP'),\n",
       " ('while', 'ADP'),\n",
       " ('we', 'PRON'),\n",
       " ('may', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('nation', 'NOUN'),\n",
       " ('divided', 'VERB'),\n",
       " ('on', 'ADP'),\n",
       " ('policies', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('we', 'PRON'),\n",
       " ('are', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('country', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('stands', 'VERB'),\n",
       " ('united', 'ADJ'),\n",
       " ('in', 'ADP'),\n",
       " ('condemning', 'VERB'),\n",
       " ('hate', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('evil', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('all', 'DET'),\n",
       " ('of', 'ADP'),\n",
       " ('its', 'PRON'),\n",
       " ('very', 'ADV'),\n",
       " ('ugly', 'ADV'),\n",
       " ('forms', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Each', 'DET'),\n",
       " ('American', 'ADJ'),\n",
       " ('generation', 'NOUN'),\n",
       " ('passes', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('torch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('truth', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('liberty', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('justice', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('an', 'DET'),\n",
       " ('unbroken', 'ADJ'),\n",
       " ('chain', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('all', 'DET'),\n",
       " ('the', 'DET'),\n",
       " ('way', 'NOUN'),\n",
       " ('down', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('the', 'DET'),\n",
       " ('present', 'ADJ'),\n",
       " ('.', '.'),\n",
       " ('That', 'DET'),\n",
       " ('torch', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('now', 'ADV'),\n",
       " ('in', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('hands', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('we', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('use', 'VERB'),\n",
       " ('it', 'PRON'),\n",
       " ('to', 'PRT'),\n",
       " ('light', 'VERB'),\n",
       " ('up', 'PRT'),\n",
       " ('the', 'DET'),\n",
       " ('world', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRON'),\n",
       " ('am', 'VERB'),\n",
       " ('here', 'ADV'),\n",
       " ('tonight', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('deliver', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('message', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('unity', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('strength', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('it', 'PRON'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('message', 'NOUN'),\n",
       " ('deeply', 'ADV'),\n",
       " ('delivered', 'VERB'),\n",
       " ('from', 'ADP'),\n",
       " ('my', 'PRON'),\n",
       " ('heart', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DET'),\n",
       " ('new', 'ADJ'),\n",
       " ('chapter', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('American', 'ADJ'),\n",
       " ('greatness', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('now', 'ADV'),\n",
       " ('beginning', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('A', 'DET'),\n",
       " ('new', 'ADJ'),\n",
       " ('national', 'ADJ'),\n",
       " ('pride', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('sweeping', 'VERB'),\n",
       " ('across', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('Nation', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('new', 'ADJ'),\n",
       " ('surge', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('optimism', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('placing', 'VERB'),\n",
       " ('impossible', 'ADJ'),\n",
       " ('dreams', 'NOUN'),\n",
       " ('firmly', 'ADV'),\n",
       " ('within', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('grasp', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('What', 'PRON'),\n",
       " ('we', 'PRON'),\n",
       " ('are', 'VERB'),\n",
       " ('witnessing', 'VERB'),\n",
       " ('today', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('renewal', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('American', 'ADJ'),\n",
       " ('spirit', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRON'),\n",
       " ('allies', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('find', 'VERB'),\n",
       " ('that', 'DET'),\n",
       " ('America', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('once', 'ADV'),\n",
       " ('again', 'ADV'),\n",
       " ('ready', 'ADJ'),\n",
       " ('to', 'PRT'),\n",
       " ('lead', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('All', 'DET'),\n",
       " ('the', 'DET'),\n",
       " ('nations', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('world—friend', 'NOUN'),\n",
       " ('or', 'CONJ'),\n",
       " ('foe—will', 'VERB'),\n",
       " ('find', 'VERB'),\n",
       " ('that', 'DET'),\n",
       " ('America', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('strong', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('America', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('proud', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('America', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('free', 'ADJ'),\n",
       " ('.', '.'),\n",
       " ('In', 'ADP'),\n",
       " ('9', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('United', 'NOUN'),\n",
       " ('States', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('celebrate', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('250th', 'ADJ'),\n",
       " ('anniversary', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('founding', 'NOUN'),\n",
       " (':', '.'),\n",
       " ('250', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('day', 'NOUN'),\n",
       " ('we', 'PRON'),\n",
       " ('declared', 'VERB'),\n",
       " ('our', 'PRON'),\n",
       " ('independence', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('one', 'NUM'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('great', 'ADJ'),\n",
       " ('milestones', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('history', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('world', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('But', 'CONJ'),\n",
       " ('what', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('America', 'NOUN'),\n",
       " ('look', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('as', 'ADP'),\n",
       " ('we', 'PRON'),\n",
       " ('reach', 'VERB'),\n",
       " ('our', 'PRON'),\n",
       " ('250th', 'NUM'),\n",
       " ('year', 'NOUN'),\n",
       " ('?', '.'),\n",
       " ('What', 'PRON'),\n",
       " ('kind', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('country', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('we', 'PRON'),\n",
       " ('leave', 'VERB'),\n",
       " ('for', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('children', 'NOUN'),\n",
       " ('?', '.'),\n",
       " ('I', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('not', 'ADV'),\n",
       " ('allow', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('mistakes', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('recent', 'ADJ'),\n",
       " ('decades', 'NOUN'),\n",
       " ('past', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('define', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('course', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('future', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('For', 'ADP'),\n",
       " ('too', 'ADV'),\n",
       " ('long', 'ADV'),\n",
       " (',', '.'),\n",
       " ('we', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('watched', 'VERB'),\n",
       " ('our', 'PRON'),\n",
       " ('middle', 'ADJ'),\n",
       " ('class', 'NOUN'),\n",
       " ('shrink', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('we', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('exported', 'VERB'),\n",
       " ('our', 'PRON'),\n",
       " ('jobs', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('wealth', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('foreign', 'ADJ'),\n",
       " ('countries', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('financed', 'VERB'),\n",
       " ('and', 'CONJ'),\n",
       " ('built', 'VERB'),\n",
       " ('one', 'NUM'),\n",
       " ('global', 'ADJ'),\n",
       " ('project', 'NOUN'),\n",
       " ('after', 'ADP'),\n",
       " ('another', 'DET'),\n",
       " (',', '.'),\n",
       " ('but', 'CONJ'),\n",
       " ('ignored', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('fates', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('children', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('inner', 'ADJ'),\n",
       " ('cities', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Chicago', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Baltimore', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Detroit', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('so', 'ADV'),\n",
       " ('many', 'ADJ'),\n",
       " ('other', 'ADJ'),\n",
       " ('places', 'NOUN'),\n",
       " ('throughout', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('land', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('defended', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('borders', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('other', 'ADJ'),\n",
       " ('nations', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('while', 'ADP'),\n",
       " ('leaving', 'VERB'),\n",
       " ('our', 'PRON'),\n",
       " ('own', 'ADJ'),\n",
       " ('borders', 'NOUN'),\n",
       " ('wide', 'ADJ'),\n",
       " ('open', 'ADJ'),\n",
       " ('for', 'ADP'),\n",
       " ('anyone', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('cross', 'VERB'),\n",
       " ('and', 'CONJ'),\n",
       " ('for', 'ADP'),\n",
       " ('drugs', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('pour', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('at', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('now', 'ADV'),\n",
       " ('unprecedented', 'ADJ'),\n",
       " ('rate', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('we', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('spent', 'VERB'),\n",
       " ('trillions', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('trillions', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('dollars', 'NOUN'),\n",
       " ('overseas', 'ADV'),\n",
       " (',', '.'),\n",
       " ('while', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('infrastructure', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('home', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('so', 'ADV'),\n",
       " ('badly', 'ADV'),\n",
       " ('crumbled', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('Then', 'ADV'),\n",
       " (',', '.'),\n",
       " ('in', 'ADP'),\n",
       " ('2016', 'NUM'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('Earth', 'NOUN'),\n",
       " ('shifted', 'VERB'),\n",
       " ('beneath', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('feet', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('rebellion', 'NOUN'),\n",
       " ('started', 'VERB'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('quiet', 'ADJ'),\n",
       " ('protest', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('spoken', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('families', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('all', 'DET'),\n",
       " ('colors', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('creeds', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('families', 'NOUN'),\n",
       " ('who', 'PRON'),\n",
       " ('just', 'ADV'),\n",
       " ('wanted', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('fair', 'ADJ'),\n",
       " ('shot', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('their', 'PRON'),\n",
       " ('children', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('fair', 'ADJ'),\n",
       " ('hearing', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('their', 'PRON'),\n",
       " ('concerns', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('But', 'CONJ'),\n",
       " ('then', 'ADV'),\n",
       " ('the', 'DET'),\n",
       " ('quiet', 'ADJ'),\n",
       " ('voices', 'NOUN'),\n",
       " ('became', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('loud', 'ADJ'),\n",
       " ('chorus', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('as', 'ADP'),\n",
       " ('thousands', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('citizens', 'NOUN'),\n",
       " ('now', 'ADV'),\n",
       " ('spoke', 'VERB'),\n",
       " ('out', 'PRT'),\n",
       " ('together', 'ADV'),\n",
       " (',', '.'),\n",
       " ('from', 'ADP'),\n",
       " ('cities', 'NOUN'),\n",
       " ('small', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('large', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('all', 'DET'),\n",
       " ('across', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('country', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Finally', 'ADV'),\n",
       " (',', '.'),\n",
       " ('the', 'DET'),\n",
       " ('chorus', 'NOUN'),\n",
       " ('became', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('earthquake', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('people', 'NOUN'),\n",
       " ('turned', 'VERB'),\n",
       " ('out', 'PRT'),\n",
       " ('by', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('tens', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('millions', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('they', 'PRON'),\n",
       " ('were', 'VERB'),\n",
       " ('all', 'DET'),\n",
       " ('united', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('one', 'NUM'),\n",
       " ('very', 'ADV'),\n",
       " ('simple', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('but', 'CONJ'),\n",
       " ('crucial', 'ADJ'),\n",
       " ('demand', 'NOUN'),\n",
       " (':', '.'),\n",
       " ('that', 'DET'),\n",
       " ('America', 'NOUN'),\n",
       " ('must', 'VERB'),\n",
       " ('put', 'VERB'),\n",
       " ('its', 'PRON'),\n",
       " ('own', 'ADJ'),\n",
       " ('citizens', 'NOUN'),\n",
       " ('first', 'ADV'),\n",
       " ('.', '.'),\n",
       " ('Because', 'ADP'),\n",
       " ('only', 'ADV'),\n",
       " ('then', 'ADV'),\n",
       " ('can', 'VERB'),\n",
       " ('we', 'PRON'),\n",
       " ('truly', 'ADV'),\n",
       " ('make', 'VERB'),\n",
       " ('America', 'NOUN'),\n",
       " ('great', 'ADJ'),\n",
       " ('again', 'ADV'),\n",
       " ('.', '.'),\n",
       " ('Dying', 'VERB'),\n",
       " ('industries', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('come', 'VERB'),\n",
       " ('roaring', 'VERB'),\n",
       " ('back', 'ADV'),\n",
       " ('to', 'PRT'),\n",
       " ('life', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Heroic', 'NOUN'),\n",
       " ('veterans', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('get', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('care', 'NOUN'),\n",
       " ('they', 'PRON'),\n",
       " ('so', 'ADV'),\n",
       " ('desperately', 'ADV'),\n",
       " ('need', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRON'),\n",
       " ('military', 'ADJ'),\n",
       " ('will', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('given', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('resources', 'NOUN'),\n",
       " ('its', 'PRON'),\n",
       " ('brave', 'ADJ'),\n",
       " ('warriors', 'NOUN'),\n",
       " ('so', 'ADV'),\n",
       " ('richly', 'ADV'),\n",
       " ('deserve', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Crumbling', 'VERB'),\n",
       " ('infrastructure', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('replaced', 'VERB'),\n",
       " ('with', 'ADP'),\n",
       " ('new', 'ADJ'),\n",
       " ('roads', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('bridges', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('tunnels', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('airports', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('railways', 'NOUN'),\n",
       " ('gleaming', 'VERB'),\n",
       " ('across', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('very', 'ADV'),\n",
       " (',', '.'),\n",
       " ('very', 'ADV'),\n",
       " ('beautiful', 'ADJ'),\n",
       " ('land', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRON'),\n",
       " ('terrible', 'ADJ'),\n",
       " ('drug', 'NOUN'),\n",
       " ('epidemic', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('slow', 'VERB'),\n",
       " ('down', 'ADV'),\n",
       " ('and', 'CONJ'),\n",
       " (',', '.'),\n",
       " ('ultimately', 'ADV'),\n",
       " (',', '.'),\n",
       " ('stop', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('our', 'PRON'),\n",
       " ('neglected', 'ADJ'),\n",
       " ('inner', 'ADJ'),\n",
       " ('cities', 'NOUN'),\n",
       " ('will', 'VERB'),\n",
       " ('see', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('rebirth', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('hope', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('safety', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('opportunity', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Above', 'ADP'),\n",
       " ('all', 'DET'),\n",
       " ('else', 'ADV'),\n",
       " (',', '.'),\n",
       " ('we', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('keep', 'VERB'),\n",
       " ('our', 'PRON'),\n",
       " ('promises', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('the', 'DET'),\n",
       " ('American', 'ADJ'),\n",
       " ('people', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('[', 'NOUN'),\n",
       " ('Applause', 'NOUN'),\n",
       " (']', 'NOUN'),\n",
       " ('Thank', 'NOUN'),\n",
       " ('you', 'PRON'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRON'),\n",
       " (\"'s\", 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('little', 'ADV'),\n",
       " ('over', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('month', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('my', 'PRON'),\n",
       " ('Inauguration', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('I', 'PRON'),\n",
       " ('want', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('take', 'VERB'),\n",
       " ('this', 'DET'),\n",
       " ('moment', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('update', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('Nation', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('progress', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('made', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('keeping', 'VERB'),\n",
       " ('those', 'DET'),\n",
       " ('promises', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Since', 'ADP'),\n",
       " ('my', 'PRON'),\n",
       " ('election', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Ford', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Fiat', 'NOUN'),\n",
       " ('Chrysler', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('General', 'NOUN'),\n",
       " ('Motors', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Sprint', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Softbank', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Lockheed', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Intel', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('Walmart', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('many', 'ADJ'),\n",
       " ('others', 'NOUN'),\n",
       " ('have', 'VERB'),\n",
       " ('announced', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('they', 'PRON'),\n",
       " ('will', 'VERB'),\n",
       " ('invest', 'VERB'),\n",
       " ('billions', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('billions', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('dollars', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('United', 'NOUN'),\n",
       " ('States', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('will', 'VERB'),\n",
       " ('create', 'VERB'),\n",
       " ('tens', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('thousands', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('new', 'ADJ'),\n",
       " ('American', 'ADJ'),\n",
       " ('jobs', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DET'),\n",
       " ('stock', 'NOUN'),\n",
       " ('market', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('gained', 'VERB'),\n",
       " ('almost', 'ADV'),\n",
       " ('$', '.'),\n",
       " ('3', 'NUM'),\n",
       " ('trillion', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('value', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('election', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('November', 'NOUN'),\n",
       " ('8—a', 'NUM'),\n",
       " ('record', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " (\"'ve\", 'VERB'),\n",
       " ('saved', 'VERB'),\n",
       " ('taxpayers', 'NOUN'),\n",
       " ('hundreds', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('millions', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('dollars', 'NOUN'),\n",
       " ('by', 'ADP'),\n",
       " ('bringing', 'VERB'),\n",
       " ('down', 'PRT'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('fantastic—and', 'NOUN'),\n",
       " ('it', 'PRON'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('fantastic—new', 'ADJ'),\n",
       " ('F-35', 'NOUN'),\n",
       " ('jet', 'NOUN'),\n",
       " ('fighter', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('and', 'CONJ'),\n",
       " ('we', 'PRON'),\n",
       " (\"'ll\", 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('saving', 'VERB'),\n",
       " ('billions', 'NOUN'),\n",
       " ('more', 'ADV'),\n",
       " ('on', 'ADP'),\n",
       " ('contracts', 'NOUN'),\n",
       " ('all', 'DET'),\n",
       " ('across', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('Government', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " ('have', 'VERB'),\n",
       " ('placed', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('hiring', 'NOUN'),\n",
       " ('freeze', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('nonmilitary', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('nonessential', 'ADJ'),\n",
       " ('Federal', 'NOUN'),\n",
       " ('workers', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " ('have', 'VERB'),\n",
       " ('begun', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('drain', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('swamp', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('government', 'NOUN'),\n",
       " ('corruption', 'NOUN'),\n",
       " ('by', 'ADP'),\n",
       " ('imposing', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('5-year', 'ADJ'),\n",
       " ('ban', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('lobbying', 'NOUN'),\n",
       " ('by', 'ADP'),\n",
       " ('executive', 'ADJ'),\n",
       " ('branch', 'NOUN'),\n",
       " ('officials', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('lifetime', 'NOUN'),\n",
       " ('ban—', 'NOUN'),\n",
       " ('[', 'NOUN'),\n",
       " ('applause', 'ADP'),\n",
       " (']', 'NOUN'),\n",
       " ('—thank', 'NOUN'),\n",
       " ('you', 'PRON'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'VERB'),\n",
       " ('you', 'PRON'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('lifetime', 'ADJ'),\n",
       " ('ban', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('becoming', 'VERB'),\n",
       " ('lobbyists', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('foreign', 'ADJ'),\n",
       " ('government', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " ('have', 'VERB'),\n",
       " ('undertaken', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('historic', 'ADJ'),\n",
       " ('effort', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('massively', 'ADV'),\n",
       " ('reduce', 'VERB'),\n",
       " ('job-crushing', 'ADJ'),\n",
       " ('regulations', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('creating', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deregulation', 'NOUN'),\n",
       " ('Task', 'NOUN'),\n",
       " ('Force', 'NOUN'),\n",
       " ('inside', 'ADP'),\n",
       " ('of', 'ADP'),\n",
       " ('every', 'DET'),\n",
       " ('Government', 'NOUN'),\n",
       " ('agency', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CONJ'),\n",
       " ('we', 'PRON'),\n",
       " (\"'re\", 'VERB'),\n",
       " ('imposing', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('new', 'ADJ'),\n",
       " ('rule', 'NOUN'),\n",
       " ('which', 'DET'),\n",
       " ('mandates', 'VERB'),\n",
       " ('that', 'ADP'),\n",
       " ('for', 'ADP'),\n",
       " ('every', 'DET'),\n",
       " ('one', 'NUM'),\n",
       " ('new', 'ADJ'),\n",
       " ('regulation', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('two', 'NUM'),\n",
       " ('old', 'ADJ'),\n",
       " ('regulations', 'NOUN'),\n",
       " ('must', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('eliminated', 'VERB'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " (\"'re\", 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('stop', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('regulations', 'NOUN'),\n",
       " ('that', 'DET'),\n",
       " ('threaten', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('future', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('livelihood', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('our', 'PRON'),\n",
       " ('great', 'ADJ'),\n",
       " ('coal', 'NOUN'),\n",
       " ('miners', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRON'),\n",
       " ('have', 'VERB'),\n",
       " ('cleared', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('way', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('construction', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('Keystone', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Dakota', 'NOUN'),\n",
       " ('Access', 'NOUN'),\n",
       " ('pipelines', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('thereby', 'ADV'),\n",
       " ('creating', 'VERB'),\n",
       " ('tens', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('thousands', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('jobs', 'NOUN'),\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('universal_tagset')  \n",
    "  \n",
    "file = open(\"../datasets/trump.txt\", \"r\",encoding=\"utf-8\") \n",
    "trump = file.read()\n",
    "words = word_tokenize(trump)\n",
    "nltk.pos_tag(words, tagset='universal')\n",
    "# fill the gap and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "\n",
    "Please use Python list features to get the last 10 sentences and display nouns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'the same God',\n",
       " 'we',\n",
       " 'this vision',\n",
       " 'we',\n",
       " 'our 250 years',\n",
       " 'glorious freedom',\n",
       " 'we',\n",
       " 'tonight',\n",
       " 'this new chapter',\n",
       " 'American greatness',\n",
       " 'The time',\n",
       " 'small thinking',\n",
       " 'The time',\n",
       " 'trivial fights',\n",
       " 'us',\n",
       " 'We',\n",
       " 'the courage',\n",
       " 'the dreams',\n",
       " 'our hearts',\n",
       " 'the bravery',\n",
       " 'the hopes',\n",
       " 'our souls',\n",
       " 'the confidence',\n",
       " 'those hopes',\n",
       " 'those dreams',\n",
       " 'action',\n",
       " 'America',\n",
       " 'our aspirations',\n",
       " 'our fears',\n",
       " 'the future',\n",
       " 'failures',\n",
       " 'the past',\n",
       " 'our vision',\n",
       " 'our doubts',\n",
       " 'I',\n",
       " 'all citizens',\n",
       " 'this renewal',\n",
       " 'the American spirit',\n",
       " 'I',\n",
       " 'all Members',\n",
       " 'Congress',\n",
       " 'me',\n",
       " 'things',\n",
       " 'our country',\n",
       " 'I',\n",
       " 'everyone',\n",
       " 'yourselves',\n",
       " 'your future',\n",
       " 'America',\n",
       " 'you',\n",
       " 'God',\n",
       " 'you',\n",
       " 'God',\n",
       " 'the United States']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "file = open(\"../datasets/trump.txt\", \"r\",encoding='utf-8') \n",
    "trump = file.read() \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(trump)\n",
    "\n",
    "sentences = list(doc.sents)[-11:]\n",
    "noun_chunks = [chunk.text for chunk in nlp(str(sentences)).noun_chunks]\n",
    "noun_chunks\n",
    "### here comes your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
    "- ``get_features_names`` - returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      "  0. 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 0. 1. 0. 2. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 2. 1. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation.\"\"\"\n",
    "    \n",
    "    __nlp = spacy.load(\"en_core_web_sm\")\n",
    "    __bow_list = []\n",
    "    \n",
    "    # your code goes maybe also here    \n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        \"\"\"Transform list of strings into BoW array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: List[str]\n",
    "                Corpus of texts to be transforrmed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "                Matrix representation of BoW\n",
    "\n",
    "        \"\"\"\n",
    "        # your code goes here  \n",
    "        tokens = []\n",
    "        for text in corpus:\n",
    "            tokens.extend(tokenize_words(text))\n",
    "        self.__bow_list = np.unique(tokens)\n",
    "        bows = np.zeros((len(corpus), len(self.__bow_list)))\n",
    "        for i, text in enumerate(corpus):\n",
    "            words = tokenize_words(text)\n",
    "            for word in words:\n",
    "                bows[i, np.where(self.__bow_list == word)] += 1\n",
    "                \n",
    "        return bows\n",
    "      \n",
    "\n",
    "    def get_feature_names(self) -> list:\n",
    "        \"\"\"Return words corresponding to columns of matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "                Words being transformed by fit function\n",
    "\n",
    "        \"\"\"   \n",
    "        # your code goes here\n",
    "        return self.__bow_list\n",
    "\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]    \n",
    "    \n",
    "vectorizer = BagOfWords()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "\n",
    "vectorizer.get_feature_names()\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Build a 5-gram model and clean up the results.\n",
    "\n",
    "There are three tasks to do:\n",
    "1. Use 5-gram model instead of 3.\n",
    "2. Change to capital letter each first letter of a sentence.\n",
    "3. Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "Hint: for 2. and 3. implement a function called ``clean_generated()`` that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "import random\n",
    "  \n",
    "wall_street = text7.tokens\n",
    "\n",
    "import re\n",
    "\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()\n",
    "\n",
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams\n",
    "\n",
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts\n",
    "\n",
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue was printed he says 0 making it less an offender than was portrayed   He admits though 0 it is n't one of Campbell Soup better products in terms of recyclability   Campbell Soup not surprisingly does n't have any plans to advertise in future issues   We do n't think 0 it will help the overall market all that much   Having the dividend increases is a supportive element in the market outlook but I do n't think 0 there is cause for concern at the moment .\n"
     ]
    }
   ],
   "source": [
    "def clean_generated(text):\n",
    "    sentences = tokenize_sentence(text)\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence[0].upper() + sentence[1:]\n",
    "        end = sentence[-1]\n",
    "        if end in ('!', '?') and re.match(r'\\s', sentence[-2]):\n",
    "            sentence = sentence[:-1:-2] + end\n",
    "        new_sentences.append(sentence)\n",
    "    return ' '.join(new_sentences)\n",
    "        \n",
    "        \n",
    "\n",
    "N=5 # fix it for other value of N\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq = None\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "# put your code here:\n",
    "generated = clean_generated(generated)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
