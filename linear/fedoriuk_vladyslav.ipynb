{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "There are three exercises in this notebook:\n",
    "\n",
    "1. Use the cross-validation method to test the linear regression with different $\\alpha$ values, at least three.\n",
    "2. Implement a SGD method that will train the Lasso regression for 10 epochs.\n",
    "3. Extend the Fisher's classifier to work with two features. Use the class as the $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross-validation linear regression\n",
    "\n",
    "You need to change the variable ``alpha`` to be a list of alphas. Next do a loop and finally compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([188, 181, 197, 168, 167, 187, 178, 194, 140, 176, 168, 192, 173, 142, 176]).reshape(-1, 1).reshape(15,1)\n",
    "y = np.array([141, 106, 149, 59, 79, 136, 65, 136, 52, 87, 115, 140, 82, 69, 121]).reshape(-1, 1).reshape(15,1)\n",
    "\n",
    "x = np.asmatrix(np.c_[np.ones((15,1)),x])\n",
    "\n",
    "I = np.identity(2)\n",
    "alphas = np.linspace(0, 1, 100)\n",
    "predictions = []\n",
    "val_set_size = 5\n",
    "for alpha in alphas:\n",
    "    pred_for_alpha = []\n",
    "    for val_idx in range(0, len(x) - val_set_size, val_set_size):\n",
    "        val_x = x[val_idx: val_idx + val_set_size]\n",
    "        val_y = y[val_idx: val_idx + val_set_size]\n",
    "        train_x = np.vstack([x[:val_idx], x[val_idx + val_set_size:]])\n",
    "        train_y = np.vstack([y[:val_idx], y[val_idx + val_set_size:]])\n",
    "        w = np.linalg.inv(train_x.T*train_x + alpha * I)*train_x.T*train_y\n",
    "        w = w.ravel()\n",
    "        val_pred = w * val_x.T\n",
    "        pred_for_alpha.append(np.mean(np.square(val_pred.T - val_y)))\n",
    "    predictions.append(np.mean(pred_for_alpha))\n",
    "\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('predictions')\n",
    "plt.title('MSE')\n",
    "plt.scatter(alphas, predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-12.63827214,   0.68494349]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement based on the Ridge regression example, the Lasso regression.\n",
    "\n",
    "Please implement the SGD method and compare the results with the sklearn Lasso regression results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x, y):\n",
    "    x[:, 1] = (x[:, 1] - x[:, 1].mean()) / x[:, 1].std()\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    \n",
    "    batch_size = 1\n",
    "    alpha = 0.001\n",
    "    beta = 0.0001\n",
    "    w = np.zeros((2, 1))\n",
    "    \n",
    "    train_x = x[:8]\n",
    "    train_y = y[:8]\n",
    "    val_x = x[8:11]\n",
    "    val_y = y[8:11]\n",
    "    test_x = x[11:]\n",
    "    test_y = y[11:]\n",
    "    \n",
    "    \n",
    "    def loss(y_true, y_pred, w):\n",
    "        return (1/(2*len(y_true))) * np.sum(np.square(y_true - y_pred)) + beta * np.sum(np.abs(w))\n",
    "                \n",
    "    def pred(x, w):\n",
    "        return np.dot(x, w)\n",
    "    \n",
    "    def grad(x, y, w):\n",
    "        y_pred = pred(x, w)\n",
    "        dw = (-1/len(y)) * np.dot(x.T, (y - y_pred)) + beta * np.sum(np.sign(w))\n",
    "        return dw\n",
    "    \n",
    "    l = []\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        train_loss = 0\n",
    "        for batch in range(0, len(train_x), batch_size):\n",
    "            batch_x = train_x[batch:batch+batch_size]\n",
    "            batch_y = train_y[batch:batch+batch_size]\n",
    "            dw = grad(batch_x, batch_y, w)\n",
    "            w = w - alpha * dw\n",
    "            train_loss += loss(batch_y, pred(batch_x, w), w)\n",
    "        print(f'epoch: {epoch}, train loss: {train_loss}, val loss: {loss(val_y, pred(val_x, w), w)}')\n",
    "        epoch += 1\n",
    "        l.append(loss(val_y, pred(val_x, w), w))\n",
    "        if len(l) > 6 and all(l[-1] > x for x in l[-2: -7:-1]):\n",
    "            break\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 4.551084812235263, val loss: 0.4552169021317744\n",
      "epoch: 1, train loss: 4.518170114753355, val loss: 0.45031976552960506\n",
      "epoch: 2, train loss: 4.485711974596406, val loss: 0.44547615423268644\n",
      "epoch: 3, train loss: 4.453702102571164, val loss: 0.44068554881701866\n",
      "epoch: 4, train loss: 4.422132374570283, val loss: 0.43594743695775506\n",
      "epoch: 5, train loss: 4.390994828201437, val loss: 0.4312613133001123\n",
      "epoch: 6, train loss: 4.360281659485709, val loss: 0.4266266793328904\n",
      "epoch: 7, train loss: 4.329985219623799, val loss: 0.422043043264548\n",
      "epoch: 8, train loss: 4.300098011828674, val loss: 0.41750991990178143\n",
      "epoch: 9, train loss: 4.270612688223304, val loss: 0.413026830530554\n",
      "epoch: 10, train loss: 4.241522046802109, val loss: 0.40859330279952644\n",
      "epoch: 11, train loss: 4.212819028454841, val loss: 0.40420887060583705\n",
      "epoch: 12, train loss: 4.184496714051606, val loss: 0.3998730739831844\n",
      "epoch: 13, train loss: 4.156548321587755, val loss: 0.39558545899216413\n",
      "epoch: 14, train loss: 4.128967203387425, val loss: 0.391345577612813\n",
      "epoch: 15, train loss: 4.1017468433645226, val loss: 0.38715298763931566\n",
      "epoch: 16, train loss: 4.074880854339966, val loss: 0.38300725257682766\n",
      "epoch: 17, train loss: 4.048362975414025, val loss: 0.3789079415403723\n",
      "epoch: 18, train loss: 4.02218706939263, val loss: 0.3748546291557675\n",
      "epoch: 19, train loss: 3.9963471202665355, val loss: 0.37084689546254185\n",
      "epoch: 20, train loss: 3.9708372307422533, val loss: 0.3668843258187968\n",
      "epoch: 21, train loss: 3.945651619823691, val loss: 0.3629665108079765\n",
      "epoch: 22, train loss: 3.9207846204434507, val loss: 0.35909304614750515\n",
      "epoch: 23, train loss: 3.8962306771427615, val loss: 0.3552635325992525\n",
      "epoch: 24, train loss: 3.871984343799062, val loss: 0.3514775758817914\n",
      "epoch: 25, train loss: 3.848040281400224, val loss: 0.34773478658440843\n",
      "epoch: 26, train loss: 3.8243932558644875, val loss: 0.3440347800828325\n",
      "epoch: 27, train loss: 3.8010381359051517, val loss: 0.34037717645664556\n",
      "epoch: 28, train loss: 3.7779698909390933, val loss: 0.336761600408341\n",
      "epoch: 29, train loss: 3.7551835890382326, val loss: 0.3331876811839947\n",
      "epoch: 30, train loss: 3.7326743949230443, val loss: 0.32965505249551663\n",
      "epoch: 31, train loss: 3.7104375679972543, val loss: 0.3261633524444491\n",
      "epoch: 32, train loss: 3.6884684604228783, val loss: 0.322712223447281\n",
      "epoch: 33, train loss: 3.6667625152347685, val loss: 0.31930131216224555\n",
      "epoch: 34, train loss: 3.6453152644938602, val loss: 0.3159302694175712\n",
      "epoch: 35, train loss: 3.62412232747831, val loss: 0.312598750141156\n",
      "epoch: 36, train loss: 3.603179408911766, val loss: 0.30930641329163594\n",
      "epoch: 37, train loss: 3.5824822972279877, val loss: 0.30605292179081833\n",
      "epoch: 38, train loss: 3.56202686287108, val loss: 0.3028379424574517\n",
      "epoch: 39, train loss: 3.5418090566305933, val loss: 0.29966114594230464\n",
      "epoch: 40, train loss: 3.521824908010799, val loss: 0.2965222066645283\n",
      "epoch: 41, train loss: 3.5020705236334098, val loss: 0.29342080274927257\n",
      "epoch: 42, train loss: 3.482542085673071, val loss: 0.2903566159665337\n",
      "epoch: 43, train loss: 3.463235850324949, val loss: 0.28732933167120556\n",
      "epoch: 44, train loss: 3.4441481463037475, val loss: 0.28433863874431\n",
      "epoch: 45, train loss: 3.4252753733735197, val loss: 0.2813842295353835\n",
      "epoch: 46, train loss: 3.4066140009076276, val loss: 0.27846579980599445\n",
      "epoch: 47, train loss: 3.3881605664782404, val loss: 0.2755830486743686\n",
      "epoch: 48, train loss: 3.3699116744747535, val loss: 0.2727356785610995\n",
      "epoch: 49, train loss: 3.351863994750546, val loss: 0.26992339513592123\n",
      "epoch: 50, train loss: 3.334014261297483, val loss: 0.2671459072655239\n",
      "epoch: 51, train loss: 3.3163592709475918, val loss: 0.2644029269623862\n",
      "epoch: 52, train loss: 3.2988958821013714, val loss: 0.2616941693346072\n",
      "epoch: 53, train loss: 3.281621013482155, val loss: 0.2590193525367167\n",
      "epoch: 54, train loss: 3.2645316429160176, val loss: 0.25637819772144205\n",
      "epoch: 55, train loss: 3.2476248061367032, val loss: 0.2537704289924137\n",
      "epoch: 56, train loss: 3.230897595615024, val loss: 0.2511957733577886\n",
      "epoch: 57, train loss: 3.2143471594122754, val loss: 0.24865396068477402\n",
      "epoch: 58, train loss: 3.1979707000571356, val loss: 0.24614472365503215\n",
      "epoch: 59, train loss: 3.181765473445584, val loss: 0.24366779772094732\n",
      "epoch: 60, train loss: 3.1657287877633653, val loss: 0.24122292106273943\n",
      "epoch: 61, train loss: 3.1498580024305287, val loss: 0.23880983454640423\n",
      "epoch: 62, train loss: 3.1341505270675882, val loss: 0.2364282816824647\n",
      "epoch: 63, train loss: 3.118603820482872, val loss: 0.23407800858551725\n",
      "epoch: 64, train loss: 3.103215389680608, val loss: 0.2317587639345543\n",
      "epoch: 65, train loss: 3.0879827888893296, val loss: 0.22947029893405024\n",
      "epoch: 66, train loss: 3.072903618610183, val loss: 0.22721236727579233\n",
      "epoch: 67, train loss: 3.0579755246847222, val loss: 0.22498472510144307\n",
      "epoch: 68, train loss: 3.043196197381799, val loss: 0.2227871309658181\n",
      "epoch: 69, train loss: 3.028563370503144, val loss: 0.2206193458008654\n",
      "epoch: 70, train loss: 3.0140748205072625, val loss: 0.2184811328803307\n",
      "epoch: 71, train loss: 2.9997283656512743, val loss: 0.21637225778509625\n",
      "epoch: 72, train loss: 2.985521865150311, val loss: 0.21429248836917797\n",
      "epoch: 73, train loss: 2.9714532183541227, val loss: 0.21224159472636747\n",
      "epoch: 74, train loss: 2.957520363940541, val loss: 0.21021934915750717\n",
      "epoch: 75, train loss: 2.943721279125448, val loss: 0.2082255261383833\n",
      "epoch: 76, train loss: 2.9300539788889073, val loss: 0.2062599022882262\n",
      "epoch: 77, train loss: 2.916516515217133, val loss: 0.20432225633880446\n",
      "epoch: 78, train loss: 2.9031069763599695, val loss: 0.2024123691041002\n",
      "epoch: 79, train loss: 2.8898234861035585, val loss: 0.2005300234505551\n",
      "epoch: 80, train loss: 2.8766642030578855, val loss: 0.1986750042678741\n",
      "epoch: 81, train loss: 2.863627319958898, val loss: 0.1968470984403763\n",
      "epoch: 82, train loss: 2.8507110629848977, val loss: 0.19504609481888066\n",
      "epoch: 83, train loss: 2.837913691086908, val loss: 0.1932717841931179\n",
      "epoch: 84, train loss: 2.8252334953327405, val loss: 0.19152395926465435\n",
      "epoch: 85, train loss: 2.812668798264465, val loss: 0.18980241462032088\n",
      "epoch: 86, train loss: 2.8002179532690166, val loss: 0.18810694670613434\n",
      "epoch: 87, train loss: 2.7878793439616736, val loss: 0.18643735380170165\n",
      "epoch: 88, train loss: 2.7756513835821237, val loss: 0.1847934359950981\n",
      "epoch: 89, train loss: 2.7635325144028844, val loss: 0.18317499515820826\n",
      "epoch: 90, train loss: 2.751521207149805, val loss: 0.1815818349225215\n",
      "epoch: 91, train loss: 2.7396159604344064, val loss: 0.1800137606553723\n",
      "epoch: 92, train loss: 2.7278153001978227, val loss: 0.178470579436616\n",
      "epoch: 93, train loss: 2.7161177791660953, val loss: 0.17695210003573233\n",
      "epoch: 94, train loss: 2.704521976316597, val loss: 0.17545813288934586\n",
      "epoch: 95, train loss: 2.6930264963553485, val loss: 0.17398849007915743\n",
      "epoch: 96, train loss: 2.681629969205006, val loss: 0.17254298531027593\n",
      "epoch: 97, train loss: 2.670331049503306, val loss: 0.17112143388994408\n",
      "epoch: 98, train loss: 2.659128416111742, val loss: 0.16972365270664885\n",
      "epoch: 99, train loss: 2.6480207716342763, val loss: 0.16834946020960967\n",
      "epoch: 100, train loss: 2.6370068419458685, val loss: 0.16699867638863583\n",
      "epoch: 101, train loss: 2.6260853757306237, val loss: 0.1656711227543469\n",
      "epoch: 102, train loss: 2.6152551440293643, val loss: 0.1643666223187475\n",
      "epoch: 103, train loss: 2.6045149397964353, val loss: 0.16308499957614978\n",
      "epoch: 104, train loss: 2.593863577465536, val loss: 0.16182608048443645\n",
      "epoch: 105, train loss: 2.583299892524423, val loss: 0.16058969244665816\n",
      "epoch: 106, train loss: 2.5728227410982663, val loss: 0.15937566429295708\n",
      "epoch: 107, train loss: 2.5624309995415135, val loss: 0.1581838262628113\n",
      "epoch: 108, train loss: 2.552123564038066, val loss: 0.15701400998759324\n",
      "epoch: 109, train loss: 2.5418993502096034, val loss: 0.1558660484734351\n",
      "epoch: 110, train loss: 2.531757292731892, val loss: 0.1547397760843964\n",
      "epoch: 111, train loss: 2.5216963449589094, val loss: 0.153635028525926\n",
      "epoch: 112, train loss: 2.511715478554621, val loss: 0.15255164282861403\n",
      "epoch: 113, train loss: 2.501813683132264, val loss: 0.15148945733222696\n",
      "epoch: 114, train loss: 2.491989965900968, val loss: 0.150448311670021\n",
      "epoch: 115, train loss: 2.482243351319577, val loss: 0.14942804675332663\n",
      "epoch: 116, train loss: 2.4725728807575154, val loss: 0.14842850475640104\n",
      "epoch: 117, train loss: 2.462977612162554, val loss: 0.14744952910154116\n",
      "epoch: 118, train loss: 2.4534566197353382, val loss: 0.1464909644444529\n",
      "epoch: 119, train loss: 2.44400899361054, val loss: 0.14555265665987185\n",
      "epoch: 120, train loss: 2.4346338395444915, val loss: 0.14463445282742915\n",
      "epoch: 121, train loss: 2.4253302786091755, val loss: 0.14373620121775885\n",
      "epoch: 122, train loss: 2.4160974468924317, val loss: 0.14285775127884154\n",
      "epoch: 123, train loss: 2.4069344952042635, val loss: 0.1419989536225792\n",
      "epoch: 124, train loss: 2.3978405887891108, val loss: 0.1411596600115966\n",
      "epoch: 125, train loss: 2.3888149070439724, val loss: 0.140339723346266\n",
      "epoch: 126, train loss: 2.379856643242251, val loss: 0.13953899765194816\n",
      "epoch: 127, train loss: 2.3709650042632107, val loss: 0.13875733806644863\n",
      "epoch: 128, train loss: 2.362139210326926, val loss: 0.13799460082768183\n",
      "epoch: 129, train loss: 2.3533784947346192, val loss: 0.13725064326154102\n",
      "epoch: 130, train loss: 2.344682103614257, val loss: 0.13652532376996926\n",
      "epoch: 131, train loss: 2.3360492956713204, val loss: 0.13581850181922686\n",
      "epoch: 132, train loss: 2.3274793419446285, val loss: 0.13513003792835243\n",
      "epoch: 133, train loss: 2.3189715255671097, val loss: 0.13445979365781333\n",
      "epoch: 134, train loss: 2.3105251415314356, val loss: 0.13380763159834108\n",
      "epoch: 135, train loss: 2.3021394964603985, val loss: 0.13317341535994914\n",
      "epoch: 136, train loss: 2.293813908381944, val loss: 0.13255700956112865\n",
      "epoch: 137, train loss: 2.2855477065087673, val loss: 0.13195827981821906\n",
      "epoch: 138, train loss: 2.2773402310223694, val loss: 0.13137709273495005\n",
      "epoch: 139, train loss: 2.2691908328614954, val loss: 0.13081331589215117\n",
      "epoch: 140, train loss: 2.2610988735148454, val loss: 0.13026681783762648\n",
      "epoch: 141, train loss: 2.2530637248179968, val loss: 0.12973746807619008\n",
      "epoch: 142, train loss: 2.24508476875442, val loss: 0.12922513705986005\n",
      "epoch: 143, train loss: 2.2371613972605373, val loss: 0.12872969617820787\n",
      "epoch: 144, train loss: 2.2292930120347108, val loss: 0.12825101774885883\n",
      "epoch: 145, train loss: 2.221479024350107, val loss: 0.127788975008143\n",
      "epoch: 146, train loss: 2.2137188548713294, val loss: 0.12734344210189077\n",
      "epoch: 147, train loss: 2.20601193347477, val loss: 0.12691429407637297\n",
      "epoch: 148, train loss: 2.1983576990725884, val loss: 0.12650140686938072\n",
      "epoch: 149, train loss: 2.1907555994402403, val loss: 0.12610465730144352\n",
      "epoch: 150, train loss: 2.183205091047495, val loss: 0.1257239230671818\n",
      "epoch: 151, train loss: 2.175705638892862, val loss: 0.1253590827267929\n",
      "epoch: 152, train loss: 2.1682567163413617, val loss: 0.12501001569766573\n",
      "epoch: 153, train loss: 2.160857804965565, val loss: 0.12467660224612377\n",
      "epoch: 154, train loss: 2.1535083943898425, val loss: 0.1243587234792927\n",
      "epoch: 155, train loss: 2.1462079821377524, val loss: 0.12405626133708991\n",
      "epoch: 156, train loss: 2.138956073482504, val loss: 0.12376909858433524\n",
      "epoch: 157, train loss: 2.1317521813004365, val loss: 0.12349711880297838\n",
      "epoch: 158, train loss: 2.124595825927448, val loss: 0.1232402063844427\n",
      "epoch: 159, train loss: 2.117486535018316, val loss: 0.1229982465220818\n",
      "epoch: 160, train loss: 2.110423843408843, val loss: 0.12277112520374733\n",
      "epoch: 161, train loss: 2.1034072929807914, val loss: 0.12255872920446606\n",
      "epoch: 162, train loss: 2.096436432529517, val loss: 0.1223609460792236\n",
      "epoch: 163, train loss: 2.089510817634277, val loss: 0.1221776641558529\n",
      "epoch: 164, train loss: 2.0826300105311417, val loss: 0.12200877252802622\n",
      "epoch: 165, train loss: 2.0757935799884537, val loss: 0.12185416104834691\n",
      "epoch: 166, train loss: 2.069001101184797, val loss: 0.12171372032154142\n",
      "epoch: 167, train loss: 2.062252155589411, val loss: 0.12158734169774758\n",
      "epoch: 168, train loss: 2.0555463308450053, val loss: 0.12147491726589835\n",
      "epoch: 169, train loss: 2.048883220652927, val loss: 0.12137633984719912\n",
      "epoch: 170, train loss: 2.042262424660628, val loss: 0.12129150298869673\n",
      "epoch: 171, train loss: 2.0356835483513915, val loss: 0.12122030095693838\n",
      "epoch: 172, train loss: 2.029146202936263, val loss: 0.12116262873171861\n",
      "epoch: 173, train loss: 2.022649938467677, val loss: 0.12111837991485269\n",
      "epoch: 174, train loss: 2.0161945220758084, val loss: 0.12108760114797575\n",
      "epoch: 175, train loss: 2.00977922551449, val loss: 0.12107003669213458\n",
      "epoch: 176, train loss: 2.0034032195676934, val loss: 0.12106559338350799\n",
      "epoch: 177, train loss: 1.9970665443813107, val loss: 0.12107417138305933\n",
      "epoch: 178, train loss: 1.990769240231609, val loss: 0.12109566974802419\n",
      "epoch: 179, train loss: 1.9845109543316184, val loss: 0.12112998820854615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-0.00452186],\n",
       "        [ 0.6530265 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([188, 181, 197, 168, 167, 187, 178, 194, 140, 176, 168, 192, 173, 142, 176]).reshape(-1, 1).reshape(15,1)\n",
    "y = np.array([141, 106, 149, 59, 79, 136, 65, 136, 52, 87, 115, 140, 82, 69, 121]).reshape(-1, 1).reshape(15,1)\n",
    "\n",
    "x = np.asmatrix(np.c_[np.ones((15,1)),x])\n",
    "\n",
    "w = sgd(x, y)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extend the Fisher's classifier\n",
    "\n",
    "Please extend the targets of the ``iris_data`` variable and use it as the $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "iris_df = pd.DataFrame(iris_data.data,columns=iris_data.feature_names)\n",
    "iris_df.head()\n",
    "\n",
    "x = iris_df[['sepal width (cm)', 'sepal length (cm)']].values # change here\n",
    "y = pd.DataFrame(iris_data.target).values # change here\n",
    "\n",
    "dataset_size = np.size(x)\n",
    "\n",
    "mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "\n",
    "SS_xy = np.sum(y * x) - dataset_size * mean_y * mean_x\n",
    "SS_xx = np.sum(x * x) - dataset_size * mean_x * mean_x\n",
    "\n",
    "a = SS_xy / SS_xx\n",
    "b = mean_y - a * mean_x\n",
    "\n",
    "\n",
    "y_pred = a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92478522, 1.05141831],\n",
       "       [0.88521238, 1.03558917],\n",
       "       [0.90104152, 1.01976004],\n",
       "       [0.89312695, 1.01184547],\n",
       "       [0.93269979, 1.04350374],\n",
       "       [0.95644349, 1.07516201],\n",
       "       [0.91687065, 1.01184547],\n",
       "       [0.91687065, 1.04350374],\n",
       "       [0.87729781, 0.99601633],\n",
       "       [0.89312695, 1.03558917],\n",
       "       [0.94061436, 1.07516201],\n",
       "       [0.91687065, 1.02767461],\n",
       "       [0.88521238, 1.02767461],\n",
       "       [0.88521238, 0.98810177],\n",
       "       [0.96435806, 1.10682029],\n",
       "       [0.99601633, 1.09890572],\n",
       "       [0.95644349, 1.07516201],\n",
       "       [0.92478522, 1.05141831],\n",
       "       [0.94852893, 1.09890572],\n",
       "       [0.94852893, 1.05141831],\n",
       "       [0.91687065, 1.07516201],\n",
       "       [0.94061436, 1.05141831],\n",
       "       [0.93269979, 1.01184547],\n",
       "       [0.90895609, 1.05141831],\n",
       "       [0.91687065, 1.02767461],\n",
       "       [0.88521238, 1.04350374],\n",
       "       [0.91687065, 1.04350374],\n",
       "       [0.92478522, 1.05933288],\n",
       "       [0.91687065, 1.05933288],\n",
       "       [0.90104152, 1.01976004],\n",
       "       [0.89312695, 1.02767461],\n",
       "       [0.91687065, 1.07516201],\n",
       "       [0.97227263, 1.05933288],\n",
       "       [0.9801872 , 1.08307658],\n",
       "       [0.89312695, 1.03558917],\n",
       "       [0.90104152, 1.04350374],\n",
       "       [0.92478522, 1.08307658],\n",
       "       [0.93269979, 1.03558917],\n",
       "       [0.88521238, 0.99601633],\n",
       "       [0.91687065, 1.05141831],\n",
       "       [0.92478522, 1.04350374],\n",
       "       [0.82981041, 1.0039309 ],\n",
       "       [0.90104152, 0.99601633],\n",
       "       [0.92478522, 1.04350374],\n",
       "       [0.94852893, 1.05141831],\n",
       "       [0.88521238, 1.02767461],\n",
       "       [0.94852893, 1.05141831],\n",
       "       [0.90104152, 1.01184547],\n",
       "       [0.94061436, 1.06724745],\n",
       "       [0.90895609, 1.04350374],\n",
       "       [0.90104152, 1.2017951 ],\n",
       "       [0.90104152, 1.15430769],\n",
       "       [0.89312695, 1.19388053],\n",
       "       [0.82981041, 1.08307658],\n",
       "       [0.86938325, 1.16222226],\n",
       "       [0.86938325, 1.09890572],\n",
       "       [0.90895609, 1.14639313],\n",
       "       [0.83772498, 1.03558917],\n",
       "       [0.87729781, 1.17013683],\n",
       "       [0.86146868, 1.05933288],\n",
       "       [0.8060667 , 1.04350374],\n",
       "       [0.88521238, 1.11473485],\n",
       "       [0.82189584, 1.12264942],\n",
       "       [0.87729781, 1.13056399],\n",
       "       [0.87729781, 1.09099115],\n",
       "       [0.89312695, 1.1780514 ],\n",
       "       [0.88521238, 1.09099115],\n",
       "       [0.86146868, 1.10682029],\n",
       "       [0.82189584, 1.13847856],\n",
       "       [0.84563954, 1.09099115],\n",
       "       [0.90104152, 1.11473485],\n",
       "       [0.86938325, 1.13056399],\n",
       "       [0.84563954, 1.14639313],\n",
       "       [0.86938325, 1.13056399],\n",
       "       [0.87729781, 1.15430769],\n",
       "       [0.88521238, 1.17013683],\n",
       "       [0.86938325, 1.18596596],\n",
       "       [0.88521238, 1.1780514 ],\n",
       "       [0.87729781, 1.12264942],\n",
       "       [0.85355411, 1.09890572],\n",
       "       [0.83772498, 1.08307658],\n",
       "       [0.83772498, 1.08307658],\n",
       "       [0.86146868, 1.10682029],\n",
       "       [0.86146868, 1.12264942],\n",
       "       [0.88521238, 1.07516201],\n",
       "       [0.91687065, 1.12264942],\n",
       "       [0.89312695, 1.1780514 ],\n",
       "       [0.82981041, 1.14639313],\n",
       "       [0.88521238, 1.09099115],\n",
       "       [0.84563954, 1.08307658],\n",
       "       [0.85355411, 1.08307658],\n",
       "       [0.88521238, 1.13056399],\n",
       "       [0.85355411, 1.10682029],\n",
       "       [0.82981041, 1.04350374],\n",
       "       [0.86146868, 1.09099115],\n",
       "       [0.88521238, 1.09890572],\n",
       "       [0.87729781, 1.09890572],\n",
       "       [0.87729781, 1.13847856],\n",
       "       [0.84563954, 1.05141831],\n",
       "       [0.86938325, 1.09890572],\n",
       "       [0.90895609, 1.14639313],\n",
       "       [0.86146868, 1.10682029],\n",
       "       [0.88521238, 1.20970967],\n",
       "       [0.87729781, 1.14639313],\n",
       "       [0.88521238, 1.16222226],\n",
       "       [0.88521238, 1.24928251],\n",
       "       [0.84563954, 1.03558917],\n",
       "       [0.87729781, 1.2255388 ],\n",
       "       [0.84563954, 1.1780514 ],\n",
       "       [0.93269979, 1.21762424],\n",
       "       [0.90104152, 1.16222226],\n",
       "       [0.86146868, 1.15430769],\n",
       "       [0.88521238, 1.18596596],\n",
       "       [0.84563954, 1.09890572],\n",
       "       [0.86938325, 1.10682029],\n",
       "       [0.90104152, 1.15430769],\n",
       "       [0.88521238, 1.16222226],\n",
       "       [0.94852893, 1.25719708],\n",
       "       [0.85355411, 1.25719708],\n",
       "       [0.82189584, 1.12264942],\n",
       "       [0.90104152, 1.19388053],\n",
       "       [0.86938325, 1.09099115],\n",
       "       [0.86938325, 1.25719708],\n",
       "       [0.86146868, 1.14639313],\n",
       "       [0.90895609, 1.1780514 ],\n",
       "       [0.90104152, 1.21762424],\n",
       "       [0.86938325, 1.13847856],\n",
       "       [0.88521238, 1.13056399],\n",
       "       [0.86938325, 1.15430769],\n",
       "       [0.88521238, 1.21762424],\n",
       "       [0.86938325, 1.23345337],\n",
       "       [0.94852893, 1.27302621],\n",
       "       [0.86938325, 1.15430769],\n",
       "       [0.86938325, 1.14639313],\n",
       "       [0.85355411, 1.13056399],\n",
       "       [0.88521238, 1.25719708],\n",
       "       [0.91687065, 1.14639313],\n",
       "       [0.89312695, 1.15430769],\n",
       "       [0.88521238, 1.12264942],\n",
       "       [0.89312695, 1.19388053],\n",
       "       [0.89312695, 1.1780514 ],\n",
       "       [0.89312695, 1.19388053],\n",
       "       [0.86146868, 1.10682029],\n",
       "       [0.90104152, 1.18596596],\n",
       "       [0.90895609, 1.1780514 ],\n",
       "       [0.88521238, 1.1780514 ],\n",
       "       [0.84563954, 1.14639313],\n",
       "       [0.88521238, 1.16222226],\n",
       "       [0.91687065, 1.13847856],\n",
       "       [0.88521238, 1.11473485]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
